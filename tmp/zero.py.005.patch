--- demodir/yesterday/zero.py	2019-12-13 16:22:23.000000000 -0600
+++ demodir/today/zero.py	2019-11-29 23:50:49.000000000 -0600
@@ -466,25 +286,87 @@ class AlphaZero:
                 "saves/" + self.model_name + "_" + str(highest_epoch) + "_" + str(highest_iter) + ".pkl")
             print("loading model at", pickle_file)
             with pickle_file.open('rb') as pickle_file:
-                computer, optim, epoch, iteration = torch.load(pickle_file,map_location=torch.device('cpu'))
+                computer, optim, epoch, iteration = torch.load(pickle_file)
             print('Loaded model at epoch ', highest_epoch, 'iteration', highest_iter)
         else:
             pickle_file = Path(task_dir).joinpath(
                 "saves/" + self.model_name + "_" + str(starting_epoch) + "_" + str(starting_iteration) + ".pkl")
             print("loading model at", pickle_file)
             with pickle_file.open('rb') as pickle_file:
-                computer, optim, epoch, iteration = torch.load(pickle_file,map_location=torch.device('cpu'))
+                computer, optim, epoch, iteration = torch.load(pickle_file)
             print('Loaded model at epoch ', starting_epoch, 'iteration', starting_iteration)
 
         self.nn.load_state_dict(computer)
-        self.optim = optim
-        self.starting_epoch = highest_epoch
-        self.starting_iter = highest_iter
+        self.optim=optim
+        self.starting_epoch=highest_epoch
+        self.starting_iter=highest_iter
 
 
 def datetime_filename():
     return datetime.datetime.now().strftime("%m-%d-%H-%M-%S")
 
+# spread more work to main thread
+# def gpu_thread_worker(nn, edge_queue, eval_batch_size, is_cuda):
+#     while True:
+#         with torch.no_grad():
+#             nn.eval()
+#             edges = []
+#             last_batch = False
+#             for i in range(eval_batch_size):
+#                 if edge_queue.empty():
+#                     break
+#                 try:
+#                     edge = edge_queue.get_nowait()
+#                     if edge is None:
+#                         last_batch = True
+#                         # print("Sentinel received. GPU will process this batch and terminate afterwards")
+#                     else:
+#                         edges.append(edge)
+#                 except queue.Empty:
+#                     pass
+#
+#             if len(edges) != 0:
+#                 # batch process
+#                 states = [edge.to_node.checker_state for edge in edges]
+#                 input_tensor = states_to_batch_tensor(states, is_cuda)
+#                 # this line is the bottleneck
+#                 if isinstance(nn, YesPolicy):
+#                     value_tensor, logits_tensor = nn(input_tensor)
+#
+#                 else:
+#                     value_tensor = nn(input_tensor)
+#
+#                 # # value
+#                 # value_array = value_tensor.cpu().numpy()
+#                 # value_array = np.squeeze(value_array, axis=1)
+#                 # value_list = value_array.tolist()
+#                 # if isinstance(nn, YesPolicy):
+#                 #     # logits
+#                 #     logit_array = logits_tensor.cpu().numpy()
+#                 #     logit_array = np.squeeze(logit_array, axis=1)
+#                 #     logit_list = logit_array.tolist()
+#                 # else:
+#                 #     logit_list = value_list
+#
+#                 for edx, edge in enumerate(edges):
+#                     edge.value = value_tensor[edx, 0].item()
+#                     if isinstance(nn, YesPolicy):
+#                         edge.logit = logits_tensor[edx,0].item()
+#                     else:
+#                         edge.logit = value_tensor[edx,0].item()
+#                     edge_queue.task_done()
+#                     edge.from_node.unassigned -= 1
+#                     if edge.from_node.unassigned == 0:
+#                         edge.from_node.lock.release()
+#             else:
+#                 time.sleep(0.1)
+#
+#             if last_batch:
+#                 edge_queue.task_done()
+#                 # print("Queue task done signal sent. Queue will join. Thread may still be running.")
+#                 return
+
+
 
 def gpu_thread_worker(nn, edge_queue, eval_batch_size, is_cuda):
     while True:
