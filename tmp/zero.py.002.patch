--- demodir/yesterday/zero.py	2019-12-13 16:22:23.000000000 -0600
+++ demodir/today/zero.py	2019-11-29 23:50:49.000000000 -0600
@@ -34,290 +31,135 @@ class AlphaZero:
     def __init__(self, model_name, is_cuda=True):
         # NEURAL NETWORK
         self.model_name = model_name
-        self.scale = 128
-
-        self.nn = SharedPolicy(self.scale)
+        self.nn = YesPolicy()
         self.is_cuda = is_cuda
         if self.is_cuda:
             self.nn = self.nn.cuda()
+        # TODO
         self.loss_fn = PaperLoss()
         self.optim = torch.optim.Adam(self.nn.parameters(), weight_decay=0.01)
         # control how many time steps each loss.backwards() is called for.
         # controls the GPU memory allocation
-        self.time_step_sample_size = 1024
+        self.training_time_step_batch_size = 2  # 16
         # controls how many boards is fed into a neural network at once
-        # controls the GPU utilization.
-        self.nn_feeding_batch_size = 512
+        # controls the speed of gpu computation.
+        self.neural_network_batch_size = 128
         # time steps contain up to self.game_size different games.
-        self.training_time_steps = []
-        self.validation_time_steps = []
-        # determines the CPU memory consumption
-        game_sacle = 2
-        self.training_games_per_refresh = 7 * game_sacle
-        self.validation_games_per_refresh = game_sacle
-        # controls the variance versus the training speed,
-        # higher means lower variance but slower training convergence due to bias
-        print("Total threads: ", self.training_games_per_refresh+self.validation_games_per_refresh)
-
-        # variance is too big. The epochs oscillate between modes
-        self.replace_ratio_per_refresh = 1 / 10
-        self.value_policy_backward_coeff = 1
-
-        self.total_game_refresh = 200
-        # if training too much, the model might diverge.
-        self.sample_batches_per_epoch = 102400 // self.time_step_sample_size * game_sacle
-        self.validation_period = 2000
-        self.total_validation_batches = 40
+        self.time_steps = []
+        self.game_size = 4
+        self.total_game_refresh = 20
+        self.reuse_game_interval = 2000
+        self.validation_period = 100
         self.print_period = 10
         self.save_period = 1000
         self.log_file = "log/" + self.model_name + "_" + datetime_filename() + ".txt"
         self.log_file = Path(self.log_file)
+        self.refresh_period = 10
+
+        self.starting_epoch=0
+        self.starting_iteration=0
+        self.load_model()
 
         # Pass to MCTS and other methods
         self.max_game_length = 200
-        self.peace = 100
         self.simulations_per_play = 200
         # this is a tuned parameter, do not change
-        # 4096 memory bound
-        self.eval_batch_size = 819200 // self.simulations_per_play
-        print("GPU memory batches:", self.eval_batch_size)
+        self.eval_batch_size = 204800 // self.simulations_per_play
         self.debug = True
-        self.max_queue_size = self.eval_batch_size * 2
+        self.max_queue_size = self.eval_batch_size*2
 
-        # if seeds are set incorrectly, the model might diverge
-        # self.seed = 123
-        # random.seed(self.seed)
-        # np.random.seed(self.seed)
-        # torch.manual_seed(self.seed)
-
-        self.fast = False
-        if self.fast:
-            self.fast_settings()
-
-        self.starting_epoch = 0
-        self.starting_iteration = 0
-        if not self.fast:
-            self.load_model()
+        # self.fast_settings()
 
     def fast_settings(self):
-        self.max_game_length = 4
-        self.simulations_per_play = 10
-        self._tsss = self.time_step_sample_size
-        self.time_step_sample_size = 4
+        self.max_game_length=4
+        self.simulations_per_play=10
+        self.game_size=2
 
-    def mcts_add_game(self, epoch):
+    def mcts_refresh_game(self):
         with torch.no_grad():
             self.nn.eval()
-            new_train_time_steps = []
-            new_validation_time_steps = []
-            nn_thread_edge_queue = queue.Queue(maxsize=self.max_queue_size)
-            # def gpu_thread_worker(nn, queue, eval_batch_size, is_cuda):
-            gpu_thread = threading.Thread(target=gpu_thread_worker,
-                                          args=(self.nn, nn_thread_edge_queue, self.eval_batch_size, self.is_cuda))
-            gpu_thread.start()
-
-            # 8 thread MCTS search
-            ars = []
-            mcts_pool = ThreadPool(processes=self.training_games_per_refresh + self.validation_games_per_refresh)
-            # mcts_search_worker(nn_thread_edge_queue,
-            #                    self.nn, self.is_cuda,
-            #                    self.max_game_length,
-            #                    self.peace,
-            #                    self.simulations_per_play,
-            #                    self.debug, epoch, new_train_time_steps)
-            for i in range(self.training_games_per_refresh):
-                async_result = mcts_pool.apply_async(mcts_search_worker, args=(nn_thread_edge_queue,
-                                                                               self.nn, self.is_cuda,
-                                                                               self.max_game_length,
-                                                                               self.peace,
-                                                                               self.simulations_per_play,
-                                                                               self.debug, epoch, new_train_time_steps))
-                ars.append(async_result)
-            for i in range(self.validation_games_per_refresh):
-                async_result = mcts_pool.apply_async(mcts_search_worker, args=(nn_thread_edge_queue,
-                                                                               self.nn, self.is_cuda,
-                                                                               self.max_game_length,
-                                                                               self.peace,
-                                                                               self.simulations_per_play,
-                                                                               self.debug, epoch,
-                                                                               new_validation_time_steps))
-                ars.append(async_result)
-
-            mcts_pool.close()
-            for ar in ars:
-                ar.wait()
-            mcts_pool.join()
-            print("MCTS pool has joined")
-
-            nn_thread_edge_queue.put(None)
-            print("Terminal sentinel is put on queue")
-            nn_thread_edge_queue.join()
-            if self.debug:
-                print("Queue has joined")
-            gpu_thread.join()
-            if self.debug:
-                print("GPU Thread has joined")
-            # new_time_steps += mcts.time_steps
-            print("Successful generation of", self.validation_games_per_refresh + self.training_games_per_refresh,
-                  "games")
-            print("Queue empty:", nn_thread_edge_queue.empty())
-            # check if any time step do not have children
-            new_train_time_steps = [ts for ts in new_train_time_steps if len(ts.children_states) != 0]
-            new_validation_time_steps = [ts for ts in new_validation_time_steps if len(ts.children_states) != 0]
-
-            # perform validation and training split
-            # all_indices=list(range(len(new_train_time_steps)))
-            # random.shuffle(all_indices)
-            # total_valid_points=int(len(new_train_time_steps)*self.validation_split)
-            # new_valid_indices=all_indices[0:total_valid_points]
-            # new_train_indices=all_indices[total_valid_points:]
-            # new_valid_points=[new_train_time_steps[i] for i in new_valid_indices]
-            # new_train_points=[new_train_time_steps[i] for i in new_train_indices]
-
-            # append training
-            self.training_time_steps = self.refresh_helper(new_train_time_steps, self.training_time_steps)
-            # old_remove= len(new_train_points) + len(self.training_time_steps) - self.replace_ratio_per_refresh
-            # if old_remove<0:
-            #     # always remove 10% of the games
-            #     # running keep 160 games per sampling population
-            #     old_remove= len(self.training_time_steps) // 10
-            # old_retain= len(self.training_time_steps) - old_remove
-            # self.training_time_steps=random.sample(self.training_time_steps, k=old_retain)
-            # self.training_time_steps= self.training_time_steps + new_train_points
-
-            # append validation
-            self.validation_time_steps = self.refresh_helper(new_validation_time_steps, self.validation_time_steps)
-
-            if not self.fast:
-                self.save_games()
-
-    def refresh_helper(self, new_points, old_points):
-        # always remove 10% of the games
-        # running keep 160 games per sampling population
-        old_remove = int(len(old_points) * self.replace_ratio_per_refresh)
-        old_retain_num = len(old_points) - old_remove
-        old_points = random.sample(old_points, k=old_retain_num)
-        old_points = old_points + new_points
-        return old_points
-
-    def save_games(self):
-        if len(self.training_time_steps)==0:
-            raise ValueError()
-        with open("training_timesteps", "wb") as f:
-            pickle.dump(self.training_time_steps, f)
-
-        with open("validation_timesteps", "wb") as f:
-            pickle.dump(self.validation_time_steps, f)
-
-    def load_games(self):
-        try:
-            with open("training_timesteps", "rb") as f:
-                self.training_time_steps = pickle.load(f)
-                print("games loaded")
-        except FileNotFoundError:
-            print("Training timestep absent from loading")
-
-        try:
-            with open("validation_timesteps", "rb") as f:
-                self.validation_time_steps = pickle.load(f)
-        except FileNotFoundError:
-            print("Validation timestep absent from loading")
+            self.time_steps = []
+            for i in range(self.game_size):
+                nn_thread_edge_queue = queue.Queue(maxsize=self.max_queue_size)
+                # def gpu_thread_worker(nn, queue, eval_batch_size, is_cuda):
+                gpu_thread = threading.Thread(target=gpu_thread_worker,
+                                              args=(self.nn, nn_thread_edge_queue, self.eval_batch_size, self.is_cuda))
+                gpu_thread.start()
+                mcts = MCTS(nn_thread_edge_queue, self.nn, self.is_cuda,
+                            self.max_game_length, self.simulations_per_play,
+                            self.debug)
+                mcts.play_until_terminal()
+                nn_thread_edge_queue.put(None)
+                # print("Terminal sentinel is put on queue")
+                nn_thread_edge_queue.join()
+                if self.debug:
+                    print("Queue has joined")
+                gpu_thread.join()
+                if self.debug:
+                    print("Thread has joined")
+                self.time_steps += mcts.time_steps
+                print("Successful generation of one game")
+                print("Queue empty:", nn_thread_edge_queue.empty())
 
     def train(self):
-        dqlen = 50
-        vdq = deque(maxlen=dqlen)
-        ptq = deque(maxlen=dqlen)
-        pdiffdq = deque(maxlen=dqlen)
-        first_run = True
         for epoch in range(self.starting_epoch, self.total_game_refresh):
-            if not self.fast:
-                self.load_games()
-                # if not first_run:
-                self.mcts_add_game(epoch)
-            else:
-                self.load_games()
-            for ti in range(self.starting_iteration, self.sample_batches_per_epoch):
-                train_vloss, train_ploss, pdiff = self.train_one_round()
-                vdq.append(train_vloss)
-                ptq.append(train_ploss)
-                pdiffdq.append(pdiff)
+            self.mcts_refresh_game()
+            for ti in range(self.starting_iteration, self.reuse_game_interval):
+                train_vloss, train_ploss = self.train_one_round()
                 if ti % self.print_period == 0:
                     self.log_print(
                         "%14s " % self.model_name +
-                        "train epoch %4d, resampling %4d. running value loss: %.5f. running policy loss: %.5f. "
-                        "running p diff: %.5f" %
-                        (epoch, ti, sum(vdq) / len(vdq), sum(ptq) / len(ptq), sum(pdiffdq) / len(pdiffdq)))
+                        "train epoch %4d, batch %4d. running value loss: %.5f. running policy loss: %.5f" %
+                        (epoch, ti, train_vloss, train_ploss))
                 if ti % self.validation_period == 0:
-                    valid_vloss, valid_ploss, valid_pdiff = self.validate()
+                    valid_vloss, valid_ploss = self.validate_one_round()
                     self.log_print(
                         "%14s " % self.model_name +
-                        "valid epoch %4d, resampling %4d. validation value loss: %.5f. validation policy loss: %.5f "
-                        "validation p diff: %.5f" %
-                        (epoch, ti, valid_vloss, valid_ploss, valid_pdiff))
+                        "valid epoch %4d, batch %4d. validation value loss: %.5f. validation policy loss: %.5f" %
+                        (epoch, ti, valid_vloss, valid_ploss))
                 if ti % self.save_period == 0:
                     self.save_model(epoch, ti)
-            self.starting_iteration = 0
-            first_run = False
 
     def run_one_round(self, sampled_tss):
         # compile value tensor
-        values = {}
-
-        value_batches = math.ceil(len(sampled_tss) / self.nn_feeding_batch_size)
+        value_batches = len(sampled_tss) // self.neural_network_batch_size + 1
         for batch_idx in range(value_batches):
             batch_tss = sampled_tss[
-                        batch_idx * self.nn_feeding_batch_size: (batch_idx + 1) * self.nn_feeding_batch_size]
+                        batch_idx * self.neural_network_batch_size: (batch_idx + 1) * self.neural_network_batch_size]
             value_inputs = [ts.checker_state for ts in batch_tss]
             value_tensor = states_to_batch_tensor(value_inputs, is_cuda=self.is_cuda)
             _, value_output = self.nn(value_tensor)
             for tsidx, ts in enumerate(batch_tss):
-                # ts.v = value_output[tsidx]
-                values[ts] = value_output[tsidx]
+                ts.v = value_output[tsidx]
 
         # compile policy tensor
         # queue up children_states
         # slice output tensor
         # tss_policy_output = {}
-
+        #
         policy_inputs_queue = []
         dim_ts = []
-        logits = {}
-        for tsidx, ts in enumerate(sampled_tss):
-            logits[ts] = []
-            # if ts is az.training_time_steps[3336]:
-            #     print("Stop here")
+        for ts in sampled_tss:
+            ts.logits = []
             for child in ts.children_states:
-                if len(policy_inputs_queue) != self.nn_feeding_batch_size + 1:
+                if len(policy_inputs_queue) != self.neural_network_batch_size:
                     # queue up
                     dim_ts.append(ts)
                     policy_inputs_queue.append(child)
                 else:
                     ### process
-                    self.get_policy_logits(policy_inputs_queue, dim_ts, logits)
+                    self.policy_bonanza(policy_inputs_queue, dim_ts)
                     policy_inputs_queue = []
                     dim_ts = []
-                    dim_ts.append(ts)
-                    policy_inputs_queue.append(child)
         # remnant in the queue
-        if len(policy_inputs_queue) != 0:
-            self.get_policy_logits(policy_inputs_queue, dim_ts, logits)
+        self.policy_bonanza(policy_inputs_queue, dim_ts)
 
         # policy transpose
         for ts in sampled_tss:
-            logits[ts] = torch.cat(logits[ts])
-        #     try:
-        #         assert(ts.logits.shape[0]==len(ts.children_states))
-        #     except AssertionError:
-        #         for tsidx, ts in enumerate(az.training_time_steps):
-        #             if ts.logits is not None:
-        #                 if ts.logits.shape[0]!=len(ts.children_states):
-        #                     print(tsidx)
-        #                     problem=True
-        # if problem:
-        #     assert False
+            ts.logits = torch.cat(ts.logits)
 
         # loss calculation
-        vloss, ploss, pdiff = 0, 0, 0
+        vloss, ploss = 0, 0
         for ts in sampled_tss:
             # should we reinitialize every time or store them?
             z = torch.Tensor([ts.z])
