--- demodir/yesterday/mcts.py	2019-12-13 16:22:23.000000000 -0600
+++ demodir/today/mcts.py	2019-11-29 23:50:49.000000000 -0600
@@ -17,7 +17,7 @@ class MCTS:
     page 2 right column paragraph 1).
     """
 
-    def __init__(self, nn_execution_queue, nn, is_cuda, max_game_length, peace, simulations_per_play, debug):
+    def __init__(self, nn_execution_queue, nn, is_cuda, max_game_length, simulations_per_play, debug):
         self.checker = Checker()
         self.permaTree = PermaTree(self.checker, is_cuda)
         self.nn_queue = nn_execution_queue
@@ -25,9 +25,8 @@ class MCTS:
         # changes to False after the first 30 moves
         self.temperature = True
         self.temperature_change_at = 30
-        self.puct = 20
+        self.puct = 10000
         self.max_game_length = max_game_length
-        self.peace = peace
         self.time_steps = []
         self.is_cuda = is_cuda
         self.simulations_per_play = simulations_per_play
@@ -40,11 +39,9 @@ class MCTS:
         when terminal, the winner is calculated, and all data points get an extra value
         :return:
         """
-        if self.debug:
-            print("Generating a new game with MCTS")
         for step in range(self.max_game_length):
-            if step % 10 == 0:
-                print("Game step " + str(step) + " /" + str(self.max_game_length))
+            # if step % 10 == 0:
+            print("Game step " + str(step) + " /" + str(self.max_game_length))
             if step == self.temperature_change_at:
                 self.temperature = False
             if self.debug:
@@ -53,38 +50,32 @@ class MCTS:
                 self.simulation()
                 # if simulation % 40 == 0 and self.debug:
                     # print("Simulation " + str(simulation) + " /" + str(self.simulations_per_play))
-            # if self.debug:
-                # t1 = time.time()
-                # print("Time per play: " + str(t1 - t0))
-                # self.permaTree.root.checker_state.print_board()
+            if self.debug:
+                t1 = time.time()
+                print("Time per play: " + str(t1 - t0))
+                self.permaTree.root.checker_state.print_board()
             terminal = self.play()
             if terminal:
                 print("Terminated at step", step)
                 break
 
         # there will be an outcome whether the game reaches terminal or not
-        # final_state = self.permaTree.root.checker_state
-        # outcome = final_state.evaluate()
-        #
-        # if outcome == 0:
-        #     z = 0
-        # else:
-        #     # truth table:
-        #     #      flipped  not flipped
-        #     # o>0    -1          1
-        #     # o<0     1         -1
-        #
-        #     a = final_state.flipped
-        #     b = outcome > 0
-        #     z = a ^ b
-        #     z = z * 2 - 1
-        #
-        # if z==1:
-        #     print("Won.")
-        #     self.permaTree.root.checker_state.print_board()
+        final_state = self.permaTree.root.checker_state
+        outcome = final_state.evaluate()
 
-        final_node=self.permaTree.root
-        z=self.find_winner(final_node)
+        # TODO not continuous outcome currently
+        if outcome == 0:
+            z = 0
+        else:
+            # truth table:
+            #      flipped  not flipped
+            # o>0    -1          1
+            # o<0     1         -1
+
+            a = final_state.flipped
+            b = outcome > 0
+            z = a ^ b
+            z = z * 2 - 1
 
         # assign z
         for ts in self.time_steps:
@@ -93,39 +84,6 @@ class MCTS:
             else:
                 ts.z = -z
 
-    def find_winner(self, mcts_node):
-        state=mcts_node.checker_state
-        outcome = state.first_player_evaluate()
-
-        # TODO not continuous outcome currently
-        if outcome == 0:
-            z = 0
-        elif outcome>0:
-            z=1
-        else:
-            z=-1
-        # else:
-        #     # truth table:
-        #     #      flipped  not flipped
-        #     # o>0    -1          1
-        #     # o<0     1         -1
-        #
-        #     a = state.flipped
-        #     b = outcome > 0
-        #     z = a ^ b
-        #     z = z * 2 - 1
-
-        if z == 1:
-            print("First player won.")
-        elif z == -1:
-            print("Second player won")
-        else:
-            assert (z == 0)
-            print("Draw")
-
-        self.permaTree.root.checker_state.print_board()
-        return z
-
     def play(self):
         """
         moves the root of the tree
@@ -139,13 +97,13 @@ class MCTS:
         root = self.permaTree.root
         if len(root.edges) == 0:
             return 1
-        visits = []
+        scores = []
         for level_one_edge in root.edges:
             vc = level_one_edge.visit_count
-            visits.append(vc)
+            scores.append(vc)
         if self.temperature:
-            sum_visits = sum(visits)
-            pi = [visit / sum_visits for visit in visits]
+            sum_scores = sum(scores)
+            pi = [score / sum_scores for score in scores]
             # samples instead
 
             sampled_action = random.choices(range(len(root.edges)), weights=pi, k=1)
@@ -154,15 +112,15 @@ class MCTS:
         else:
             maxscore = - float("inf")
             maxedx = None
-            for edx, score in enumerate(visits):
+            for edx, score in enumerate(scores):
                 if score > maxscore:
                     maxedx = edx
                     maxscore = score
-            pi = [0] * len(visits)
+            pi = [0] * len(scores)
             pi[maxedx] = 1
             max_action = root.edges[maxedx]
             self.permaTree.move_root(max_action.to_node)
-        if self.permaTree.last_capture == self.peace:
+        if self.permaTree.last_capture == 40:
             print("Terminated due to peaceful activity")
             return 2
 
@@ -206,9 +164,11 @@ class MCTS:
         # let's release the lock when logits are ready, but probability might not be computed
         node.lock.acquire()
         if not node.probability_ready:
+            # COMMENT THIS OUT TO USE OLD FUNCTION
             for edge in node.edges:
                 edge.value=edge.value.item()
                 edge.logit=edge.logit.item()
+            # COMMENT THIS OUT TO USE OLD FUNCTION
 
             # probability
             prob_tensor = self.nn.logits_to_probability(torch.Tensor([edge.logit for edge in node.edges]))
@@ -239,19 +199,9 @@ class MCTS:
                 print(node.is_root())
             Usa = (self.puct * edge.prior_probability * math.sqrt(sumnsb)) / (1 + Nsa)
             QU.append(edge.mean_action_value + Usa)
-            # if edge.is_first_player():
-            #     QU.append(edge.mean_action_value + Usa)
-            # else:
-            #     QU.append(-edge.mean_action_value+ Usa)
 
         # pick the edge that is returned by the argmax and return it
         # make it node
-        if node.is_root():
-            epsilon=0.25
-            d_samples=np.random.dirichlet((0.03,)*len(QU)).tolist()
-            for quidx in range(len(QU)):
-                QU[quidx]=QU[quidx]*(1-epsilon)+d_samples[quidx]*epsilon
-
         maxqu = -float('inf')
         maxedx = None
         for edx, qu in enumerate(QU):
@@ -290,26 +240,20 @@ class MCTS:
         trace back the whole path from given node till root node while updating edges on the path
 
         :param leaf_node:
-        :param v: the leaf's perspective value, accessed from leaf_node.from_edge
         :return:
         """
         # parent of root node is null
         current_node = leaf_node
-        leaf_first_player=leaf_node.is_first_player()
         while current_node.parent is not None:
             edge = current_node.from_edge
             edge.visit_count = edge.visit_count + 1
-            if edge.to_node.is_first_player()==leaf_first_player:
-                edge.total_action_value = edge.total_action_value - v
-            else:
-                edge.total_action_value = edge.total_action_value + v
+            edge.total_action_value = edge.total_action_value + v
             edge.mean_action_value = edge.total_action_value / edge.visit_count
             current_node = edge.from_node
 
     def print_root(self):
         self.permaTree.root.checker_state.print_board()
 
-
 # interfaces with the Zero
 class TimeStep:
     def __init__(self, checker_state, children_states, mcts_pi):
diff -up -r demodir/yesterday/neuralnetwork.py demodir/today/neuralnetwork.py
